#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script to crawl postal codes from mabuudien.net for all Vietnamese provinces
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import re
from urllib.parse import quote

# Mapping province codes to Vietnamese names (from tinh_thanhpho.php)
PROVINCES = {
    "ANGIANG": "An Giang",
    "BARIA": "B√† R·ªãa - V≈©ng T√†u",
    "BACLIEU": "B·∫°c Li√™u",
    "BACGIANG": "B·∫Øc Giang",
    "BACKAN": "B·∫Øc K·∫°n",
    "BACNINH": "B·∫Øc Ninh",
    "BENTRE": "B·∫øn Tre",
    "BINHDINH": "B√¨nh ƒê·ªãnh",
    "BINHDUONG": "B√¨nh D∆∞∆°ng",
    "BINHPHUOC": "B√¨nh Ph∆∞·ªõc",
    "BINHTHUAN": "B√¨nh Thu·∫≠n",
    "CAMAU": "C√† Mau",
    "CAOBANG": "Cao B·∫±ng",
    "CANTHO": "C·∫ßn Th∆°",
    "DANANG": "ƒê√† N·∫µng",
    "DAKLAK": "ƒê·∫Øk L·∫Øk",
    "DAKNONG": "ƒê·∫Øk N√¥ng",
    "DIENBIEN": "ƒêi·ªán Bi√™n",
    "DONGNAI": "ƒê·ªìng Nai",
    "DONGTHAP": "ƒê·ªìng Th√°p",
    "GIALAI": "Gia Lai",
    "HAGIANG": "H√† Giang",
    "HANAM": "H√† Nam",
    "HANOI": "H√† N·ªôi",
    "HATINH": "H√† Tƒ©nh",
    "HAIDUONG": "H·∫£i D∆∞∆°ng",
    "HAIPHONG": "H·∫£i Ph√≤ng",
    "HAUGIANG": "H·∫≠u Giang",
    "HOABINH": "H√≤a B√¨nh",
    "HOCHIMINH": "H·ªì Ch√≠ Minh",
    "HUNGYEN": "H∆∞ng Y√™n",
    "KHANHHOA": "Kh√°nh H√≤a",
    "KIENGIANG": "Ki√™n Giang",
    "KONTUM": "Kon Tum",
    "LAICHAU": "Lai Ch√¢u",
    "LAMDONG": "L√¢m ƒê·ªìng",
    "LANGSON": "L·∫°ng S∆°n",
    "LAOCAI": "L√†o Cai",
    "LONGAN": "Long An",
    "NAMDINH": "Nam ƒê·ªãnh",
    "NGHEAN": "Ngh·ªá An",
    "NINHBINH": "Ninh B√¨nh",
    "NINHTHUAN": "Ninh Thu·∫≠n",
    "PHUTHO": "Ph√∫ Th·ªç",
    "PHUYEN": "Ph√∫ Y√™n",
    "QUANGBINH": "Qu·∫£ng B√¨nh",
    "QUANGNAM": "Qu·∫£ng Nam",
    "QUANGNGAI": "Qu·∫£ng Ng√£i",
    "QUANGNINH": "Qu·∫£ng Ninh",
    "QUANGTRI": "Qu·∫£ng Tr·ªã",
    "SOCTRANG": "S√≥c TrƒÉng",
    "SONLA": "S∆°n La",
    "TAYNINH": "T√¢y Ninh",
    "THAIBINH": "Th√°i B√¨nh",
    "THAINGUYEN": "Th√°i Nguy√™n",
    "THANHHOA": "Thanh H√≥a",
    "THUATHIENHUE": "Th·ª´a Thi√™n Hu·∫ø",
    "TIENGIANG": "Ti·ªÅn Giang",
    "TRAVINH": "Tr√† Vinh",
    "TUYENQUANG": "Tuy√™n Quang",
    "VINHLONG": "Vƒ©nh Long",
    "VINHPHUC": "Vƒ©nh Ph√∫c",
    "YENBAI": "Y√™n B√°i"
}

def slugify_vietnamese(text):
    """Convert Vietnamese text to URL-friendly slug"""
    # Lowercase
    text = text.lower()
    
    # Replace Vietnamese characters
    replacements = {
        '√°': 'a', '√†': 'a', '·∫£': 'a', '√£': 'a', '·∫°': 'a',
        'ƒÉ': 'a', '·∫Ø': 'a', '·∫±': 'a', '·∫≥': 'a', '·∫µ': 'a', '·∫∑': 'a',
        '√¢': 'a', '·∫•': 'a', '·∫ß': 'a', '·∫©': 'a', '·∫´': 'a', '·∫≠': 'a',
        '√©': 'e', '√®': 'e', '·∫ª': 'e', '·∫Ω': 'e', '·∫π': 'e',
        '√™': 'e', '·∫ø': 'e', '·ªÅ': 'e', '·ªÉ': 'e', '·ªÖ': 'e', '·ªá': 'e',
        '√≠': 'i', '√¨': 'i', '·ªâ': 'i', 'ƒ©': 'i', '·ªã': 'i',
        '√≥': 'o', '√≤': 'o', '·ªè': 'o', '√µ': 'o', '·ªç': 'o',
        '√¥': 'o', '·ªë': 'o', '·ªì': 'o', '·ªï': 'o', '·ªó': 'o', '·ªô': 'o',
        '∆°': 'o', '·ªõ': 'o', '·ªù': 'o', '·ªü': 'o', '·ª°': 'o', '·ª£': 'o',
        '√∫': 'u', '√π': 'u', '·ªß': 'u', '≈©': 'u', '·ª•': 'u',
        '∆∞': 'u', '·ª©': 'u', '·ª´': 'u', '·ª≠': 'u', '·ªØ': 'u', '·ª±': 'u',
        '√Ω': 'y', '·ª≥': 'y', '·ª∑': 'y', '·ªπ': 'y', '·ªµ': 'y',
        'ƒë': 'd',
        ' ': '-', '/': '-'
    }
    
    for vn_char, en_char in replacements.items():
        text = text.replace(vn_char, en_char)
    
    # Remove special characters
    text = re.sub(r'[^a-z0-9\-]', '', text)
    
    # Remove multiple hyphens
    text = re.sub(r'-+', '-', text)
    
    return text.strip('-')

def crawl_province_postcodes(province_name):
    """Crawl postal codes for a specific province"""
    province_slug = slugify_vietnamese(province_name)
    url = f"https://mabuudien.net/ma-buu-dien-{province_slug}"
    
    print(f"üìç Crawling {province_name}: {url}")
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = 'utf-8'
        
        if response.status_code != 200:
            print(f"   ‚ùå Error: HTTP {response.status_code}")
            return {}
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Find the table with district postal codes
        districts = {}
        tables = soup.find_all('table')
        
        for table in tables:
            rows = table.find_all('tr')
            for row in rows:
                cols = row.find_all('td')
                if len(cols) >= 2:
                    district_name = cols[0].get_text(strip=True)
                    postcode = cols[1].get_text(strip=True)
                    
                    # Clean district name (remove "Qu·∫≠n", "Huy·ªán", "Th·ªã x√£", "Th√†nh ph·ªë")
                    district_clean = district_name
                    for prefix in ['Qu·∫≠n ', 'Huy·ªán ', 'Th·ªã x√£ ', 'Th√†nh ph·ªë ']:
                        if district_clean.startswith(prefix):
                            district_clean = district_clean[len(prefix):]
                    
                    if postcode and postcode.isdigit() and len(postcode) == 5:
                        districts[district_clean] = postcode
        
        print(f"   ‚úÖ Found {len(districts)} districts")
        return districts
        
    except Exception as e:
        print(f"   ‚ùå Error: {str(e)}")
        return {}

def load_districts_mapping():
    """Load district mapping from quan_huyen.php"""
    districts_map = {}
    
    with open('../data/quan_huyen.php', 'r', encoding='utf-8') as f:
        content = f.read()
        
        # Find district entries
        pattern = r'array\("maqh"=>"([^"]+)","name"=>"([^"]+)","matp"=>"([^"]+)"\)'
        matches = re.findall(pattern, content)
        
        for district_code, district_name, province_code in matches:
            # Clean district name
            district_clean = district_name
            for prefix in ['Qu·∫≠n ', 'Huy·ªán ', 'Th·ªã x√£ ', 'Th√†nh ph·ªë ']:
                if district_clean.startswith(prefix):
                    district_clean = district_clean[len(prefix):]
            
            key = f"{province_code}_{district_code}"
            districts_map[key] = {
                'code': district_code,
                'name': district_name,
                'clean_name': district_clean,
                'province': province_code
            }
    
    print(f"üìä Loaded {len(districts_map)} districts from PHP file")
    return districts_map

def normalize_district_name(name):
    """Normalize district name for matching"""
    # Remove accents and special chars
    normalized = slugify_vietnamese(name)
    # Remove common prefixes
    for prefix in ['quan-', 'huyen-', 'thi-xa-', 'thanh-pho-']:
        if normalized.startswith(prefix):
            normalized = normalized[len(prefix):]
    return normalized

def main():
    print("üöÄ Starting postal code crawler...")
    print("=" * 60)
    
    # Load districts mapping
    districts_map = load_districts_mapping()
    
    # Crawl postcodes for each province
    all_postcodes = {}
    
    for province_code, province_name in PROVINCES.items():
        postcodes = crawl_province_postcodes(province_name)
        
        if postcodes:
            all_postcodes[province_code] = postcodes
            time.sleep(1)  # Be nice to the server
    
    print("\n" + "=" * 60)
    print("üìä SUMMARY")
    print("=" * 60)
    print(f"Total provinces crawled: {len(all_postcodes)}")
    
    # Match postcodes with district codes
    matched_postcodes = {}
    unmatched = []
    
    for province_code, postcodes in all_postcodes.items():
        for district_name, postcode in postcodes.items():
            # Try to find matching district code
            matched = False
            normalized_name = normalize_district_name(district_name)
            
            for key, district_info in districts_map.items():
                if district_info['province'] == province_code:
                    district_normalized = normalize_district_name(district_info['clean_name'])
                    
                    if normalized_name == district_normalized:
                        matched_key = f"{province_code}_D_{district_info['code']}"
                        matched_postcodes[matched_key] = postcode
                        matched = True
                        break
            
            if not matched:
                unmatched.append(f"{province_code}: {district_name} = {postcode}")
    
    print(f"Total postcodes matched: {len(matched_postcodes)}")
    print(f"Unmatched: {len(unmatched)}")
    
    if unmatched:
        print("\n‚ö†Ô∏è  Unmatched districts:")
        for item in unmatched[:20]:  # Show first 20
            print(f"   {item}")
    
    # Save to JSON file
    output_file = 'postcodes-mapping.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(matched_postcodes, f, ensure_ascii=False, indent=2)
    
    print(f"\n‚úÖ Saved to {output_file}")
    print("=" * 60)

if __name__ == "__main__":
    main()
